from langgraph.types import Send
from typing_extensions import TypedDict, Annotated
from pydantic import BaseModel, Field
import operator
from langgraph.graph import StateGraph, START, END
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_openai import ChatOpenAI

# é…ç½® LLM
openai_api_key = "sk-nRyX1HmENCf4QEk5V0yWZKrQkIKKnEfXloy9lSOe3Jjl9AJH"
base_url = "https://api.poixe.com/v1"

llm = ChatOpenAI(
    openai_api_key=openai_api_key,
    base_url=base_url
)

# Schema for report sections
class Section(BaseModel):
    """Schema for a section of the report"""
    name: str = Field(description="Name of the section")
    description: str = Field(description="Description of what should be in the section")

class Sections(BaseModel):
    """Schema for the list of sections"""
    sections: list[Section]

# Planner LLM with structured output
planner = llm.with_structured_output(Sections)

# Graph state
class State(TypedDict):
    topic: str  # Report topic
    sections: list[Section]  # List of report sections
    completed_sections: Annotated[
        list, operator.add
    ]  # All workers write to this key in parallel
    final_report: str  # Final report


# Worker state
class WorkerState(TypedDict):
    section: Section
    completed_sections: Annotated[list, operator.add]


# Nodes
def orchestrator(state: State):
    """Orchestrator that generates a plan for the report"""

    print("\nğŸ“‹ Orchestrator å¼€å§‹åˆ¶å®šè®¡åˆ’...")

    # Generate queries
    report_sections = planner.invoke(
        [
            SystemMessage(content="Generate a plan for the report."),
            HumanMessage(content=f"Here is the report topic: {state['topic']}"),
        ]
    )

    print(f"âœ… è®¡åˆ’å®Œæˆï¼ç”Ÿæˆäº† {len(report_sections.sections)} ä¸ªç« èŠ‚:")
    for i, section in enumerate(report_sections.sections, 1):
        print(f"   {i}. {section.name}")

    return {"sections": report_sections.sections}


def llm_call(state: WorkerState):
    """Worker writes a section of the report"""

    section_name = state['section'].name
    print(f"   ğŸ”¨ Worker å¼€å§‹æ’°å†™: {section_name}")

    # Generate section
    section = llm.invoke(
        [
            SystemMessage(
                content="Write a report section following the provided name and description. Include no preamble for each section. Use markdown formatting."
            ),
            HumanMessage(
                content=f"Here is the section name: {state['section'].name} and description: {state['section'].description}"
            ),
        ]
    )

    print(f"   âœ… Worker å®Œæˆ: {section_name}")

    # å…³é”®ï¼šè¿”å›çš„å†…å®¹ä¼šé€šè¿‡ operator.add åˆå¹¶åˆ° state["completed_sections"]
    # æ¯ä¸ª worker è¿”å›ä¸€ä¸ªåˆ—è¡¨ï¼ŒLangGraph ä¼šè‡ªåŠ¨æ‰§è¡Œ:
    # completed_sections = completed_sections + [section.content]
    result = {"completed_sections": [section.content]}
    print(f"      â†’ å†™å…¥ completed_sections: [ç« èŠ‚å†…å®¹ {len(section.content)} å­—ç¬¦]")

    # Write the updated section to completed sections
    return result


def synthesizer(state: State):
    """Synthesize full report from sections"""

    print(f"\nğŸ“ Synthesizer å¼€å§‹åˆæˆæœ€ç»ˆæŠ¥å‘Š...")

    # List of completed sections
    completed_sections = state["completed_sections"]

    print(f"   æ”¶åˆ° {len(completed_sections)} ä¸ªå·²å®Œæˆçš„ç« èŠ‚")
    print(f"\n   âš ï¸  é‡è¦ï¼šç« èŠ‚çš„é¡ºåº = Workers å®Œæˆçš„é¡ºåºï¼ˆä¸æ˜¯åŸå§‹è®¡åˆ’é¡ºåºï¼‰")
    print(f"   å®é™…æ”¶é›†åˆ°çš„ç« èŠ‚é¡ºåº:")
    for i, section in enumerate(completed_sections, 1):
        # æå–ç« èŠ‚æ ‡é¢˜ï¼ˆç¬¬ä¸€è¡Œé€šå¸¸æ˜¯æ ‡é¢˜ï¼‰
        title = section.split('\n')[0].strip('#').strip()[:50]
        print(f"      {i}. {title}")

    # Format completed section to str to use as context for final sections
    completed_report_sections = "\n\n---\n\n".join(completed_sections)

    print(f"\n   âœ… æŠ¥å‘Šåˆæˆå®Œæˆ!\n")

    return {"final_report": completed_report_sections}


# Conditional edge function to create llm_call workers that each write a section of the report
def assign_workers(state: State):
    """Assign a worker to each section in the plan"""

    print(f"\nğŸš€ ä½¿ç”¨ Send() API åŠ¨æ€åˆ›å»º Workers...")
    print(f"   å…±éœ€åˆ›å»º {len(state['sections'])} ä¸ªå¹¶è¡Œçš„ llm_call å®ä¾‹\n")

    # Kick off section writing in parallel via Send() API
    # å…³é”®ï¼šæ¯ä¸ª Send å¯¹è±¡ = ä¸€ä¸ªç‹¬ç«‹çš„èŠ‚ç‚¹æ‰§è¡Œå®ä¾‹
    send_list = [Send("llm_call", {"section": s}) for s in state["sections"]]

    print(f"   âœ… å·²åˆ›å»º {len(send_list)} ä¸ª Send å¯¹è±¡")
    print(f"   â†’ LangGraph å°†å¹¶è¡Œæ‰§è¡Œè¿™äº› workers\n")

    return send_list


# Build workflow
orchestrator_worker_builder = StateGraph(State)

# Add the nodes
orchestrator_worker_builder.add_node("orchestrator", orchestrator)
orchestrator_worker_builder.add_node("llm_call", llm_call)
orchestrator_worker_builder.add_node("synthesizer", synthesizer)

# Add edges to connect nodes
orchestrator_worker_builder.add_edge(START, "orchestrator")
orchestrator_worker_builder.add_conditional_edges(
    "orchestrator", assign_workers, ["llm_call"]
)
orchestrator_worker_builder.add_edge("llm_call", "synthesizer")
orchestrator_worker_builder.add_edge("synthesizer", END)

# Compile the workflow
orchestrator_worker = orchestrator_worker_builder.compile()

# Show the workflow - ä¿å­˜ä¸ºæ–‡ä»¶
print("ç”Ÿæˆç¼–æ’-å·¥ä½œè€…å·¥ä½œæµå›¾è¡¨...")
png_data = orchestrator_worker.get_graph().draw_mermaid_png()
with open("orchestrator_worker_graph.png", "wb") as f:
    f.write(png_data)
print("âœ… å›¾è¡¨å·²ä¿å­˜åˆ°: orchestrator_worker_graph.png")
print("   ä½¿ç”¨å‘½ä»¤æŸ¥çœ‹: open orchestrator_worker_graph.png\n")

# Invoke
print("\n" + "=" * 70)
print("å¼€å§‹æ¼”ç¤º Send() API çš„åŠ¨æ€å¹¶è¡Œæ‰§è¡Œ")
print("=" * 70)
state = orchestrator_worker.invoke({"topic": "Create a report on LLM scaling laws"})

print("=" * 70)
print("æœ€ç»ˆæŠ¥å‘Š:")
print("=" * 70 + "\n")
print(state["final_report"])