"""LangGraph Weather Agent Example - Using Stable API"""

from dataclasses import dataclass
from langchain_core.tools import tool
from langchain_core.messages import SystemMessage, HumanMessage
from langgraph.checkpoint.memory import MemorySaver
# ä½¿ç”¨æ–°çš„å¯¼å…¥æ–¹å¼ï¼ˆé¿å…å¼ƒç”¨è­¦å‘Šï¼‰
try:
    from langchain.agents import create_react_agent
except ImportError:
    from langgraph.prebuilt import create_react_agent

# ===== System Prompt =====
SYSTEM_PROMPT = """You are an expert weather forecaster, who speaks in puns.

You have access to two tools:

- get_weather_for_location: use this to get the weather for a specific location
- get_user_location: use this to get the user's location

If a user asks you for the weather, make sure you know the location. If you can tell from the question that they mean wherever they are, use the get_user_location tool to find their location."""

# ===== Context Schema =====
@dataclass
class Context:
    """Custom runtime context schema."""
    user_id: str

# å…¨å±€å­˜å‚¨ç”¨æˆ·ä¸Šä¸‹æ–‡ï¼ˆç®€åŒ–ç‰ˆï¼‰
USER_CONTEXT = {}

# ===== Tools =====
@tool
def get_weather_for_location(city: str) -> str:
    """Get weather for a given city."""
    return f"It's always sunny in {city}!"

@tool
def get_user_location(user_id: str = "1") -> str:
    """Retrieve user location based on user ID.

    Args:
        user_id: The user ID (default is "1")
    """
    # ä»å…¨å±€ä¸Šä¸‹æ–‡è·å–
    ctx = USER_CONTEXT.get("current_user")
    if ctx:
        user_id = ctx.user_id
    return "Florida" if user_id == "1" else "SF"

# ===== Response Format (ç»“æ„åŒ–è¾“å‡º) =====
@dataclass
class ResponseFormat:
    """Response schema for the agent."""
    # A punny response (always required)
    punny_response: str
    # Any interesting information about the weather if available
    weather_conditions: str | None = None

# ===== Initialize Model =====
from langchain_openai import ChatOpenAI

# è®¾ç½® API é…ç½®ï¼ˆä½¿ç”¨è‡ªå®šä¹‰ä»£ç†ï¼‰
openai_api_key = "sk-nRyX1HmENCf4QEk5V0yWZKrQkIKKnEfXloy9lSOe3Jjl9AJH"
base_url = "https://api.poixe.com/v1"  # è‡ªå®šä¹‰ OpenAI å…¼å®¹ç«¯ç‚¹

model = ChatOpenAI(
    model="gpt-4",
    temperature=0.5,
    timeout=10,
    max_tokens=1000,
    openai_api_key=openai_api_key,
    base_url=base_url
)

# ===== è‡ªå®šä¹‰ Agent å·¥ä½œæµï¼ˆæ¨¡æ‹Ÿ ToolStrategyï¼‰=====
from typing import TypedDict, Annotated
from langgraph.graph import StateGraph, END
from langchain_core.messages import AIMessage, ToolMessage
import operator

# å®šä¹‰çŠ¶æ€
class AgentState(TypedDict):
    messages: Annotated[list, operator.add]
    structured_response: dict | None

# åˆ›å»ºå¸¦ç»“æ„åŒ–è¾“å‡ºçš„æ¨¡å‹
structured_llm = model.with_structured_output(ResponseFormat)

def should_continue(state: AgentState):
    """åˆ¤æ–­æ˜¯å¦ç»§ç»­è°ƒç”¨å·¥å…·"""
    last_message = state['messages'][-1]
    if hasattr(last_message, 'tool_calls') and last_message.tool_calls:
        return "tools"
    return "format_response"

def call_model(state: AgentState):
    """è°ƒç”¨æ¨¡å‹"""
    messages = [SystemMessage(content=SYSTEM_PROMPT)] + state['messages']
    response = model.invoke(messages)
    return {"messages": [response]}

def call_tools(state: AgentState):
    """æ‰§è¡Œå·¥å…·è°ƒç”¨"""
    from langgraph.prebuilt import ToolNode
    tool_node = ToolNode([get_user_location, get_weather_for_location])
    result = tool_node.invoke(state)
    return result

def format_response(state: AgentState):
    """æ ¼å¼åŒ–ä¸ºç»“æ„åŒ–è¾“å‡ºï¼ˆæ¨¡æ‹Ÿ ToolStrategyï¼‰"""
    # è·å–æœ€åçš„ AI å›å¤
    last_ai_message = None
    for msg in reversed(state['messages']):
        if isinstance(msg, AIMessage) and not hasattr(msg, 'tool_calls'):
            last_ai_message = msg
            break
        elif isinstance(msg, AIMessage) and hasattr(msg, 'tool_calls') and not msg.tool_calls:
            last_ai_message = msg
            break
        

    if not last_ai_message or not last_ai_message.content:
        # å¦‚æœæ²¡æœ‰æœ€ç»ˆå›å¤ï¼Œè®©æ¨¡å‹ç”Ÿæˆä¸€ä¸ª
        messages = state['messages']
        response = model.invoke(messages)
        last_ai_message = response

    # ä½¿ç”¨ç»“æ„åŒ–æ¨¡å‹æ ¼å¼åŒ–
    structured = structured_llm.invoke([
        SystemMessage(content="""Extract the weather forecast into structured format:
        - punny_response: The punny weather forecast message
        - weather_conditions: The actual weather condition (e.g., 'sunny', 'rainy') or None"""),
        HumanMessage(content=last_ai_message.content)
    ])

    return {
        "structured_response": structured,
        "messages": []  # ä¸æ·»åŠ æ–°æ¶ˆæ¯
    }

# æ„å»ºå›¾
workflow = StateGraph(AgentState)
workflow.add_node("agent", call_model)
workflow.add_node("tools", call_tools)
workflow.add_node("format_response", format_response)

workflow.set_entry_point("agent")
workflow.add_conditional_edges(
    "agent",
    should_continue,
    {
        "tools": "tools",
        "format_response": "format_response"
    }
)
workflow.add_edge("tools", "agent")
workflow.add_edge("format_response", END)

# ç¼–è¯‘
checkpointer = MemorySaver()
agent = workflow.compile(checkpointer=checkpointer)

# ===== Run Example 1 =====
print("=" * 60)
print("Example 1: What is the weather outside?")
print("=" * 60)

# è®¾ç½®ç”¨æˆ·ä¸Šä¸‹æ–‡
USER_CONTEXT["current_user"] = Context(user_id="1")

# thread_id æ˜¯å¯¹è¯çš„å”¯ä¸€æ ‡è¯†ç¬¦
config = {"configurable": {"thread_id": "1"}}

response = agent.invoke(
    {
        "messages": [HumanMessage(content="what is the weather outside?")],
        "structured_response": None
    },
    config=config
)

# åˆ†æå·¥å…·è°ƒç”¨è¿‡ç¨‹
print("\nğŸ“‹ å·¥å…·è°ƒç”¨è®°å½•:")
print("-" * 60)
for msg in response['messages']:
    if hasattr(msg, 'tool_calls') and msg.tool_calls:
        # AI å†³å®šè°ƒç”¨å·¥å…·
        for tool_call in msg.tool_calls:
            print(f"ğŸ”§ è°ƒç”¨å·¥å…·: {tool_call['name']}")
            print(f"   å‚æ•°: {tool_call['args']}")
    elif msg.__class__.__name__ == 'ToolMessage':
        # å·¥å…·è¿”å›ç»“æœ
        print(f"âœ… å·¥å…·è¿”å›: {msg.content}")
        print()

# æ‰“å°æœ€åçš„å›å¤
final_message = response['messages'][-1]
print("-" * 60)
print(f"\nğŸ’¬ Agent æœ€ç»ˆå›å¤: {final_message.content}\n")

# æå–ç»“æ„åŒ–è¾“å‡º
print("\nğŸ“Š ç»“æ„åŒ–è¾“å‡º (ResponseFormat):")
print("-" * 60)
structured = extract_structured_response(response)
print(f"  punny_response: \"{structured['punny_response']}\"")
print(f"  weather_conditions: {structured['weather_conditions']}")
print()

# ===== Run Example 2 =====
print("=" * 60)
print("Example 2: Continue conversation")
print("=" * 60)

# ç»§ç»­åŒä¸€ä¸ªå¯¹è¯ï¼ˆä½¿ç”¨ç›¸åŒçš„ thread_idï¼‰
response = agent.invoke(
    {
        "messages": [HumanMessage(content="thank you!")]
    },
    config=config
)

# åˆ†æå·¥å…·è°ƒç”¨
print("\nğŸ“‹ å·¥å…·è°ƒç”¨è®°å½•:")
print("-" * 60)
tool_called = False
for msg in response['messages']:
    if hasattr(msg, 'tool_calls') and msg.tool_calls:
        for tool_call in msg.tool_calls:
            print(f"ğŸ”§ è°ƒç”¨å·¥å…·: {tool_call['name']}")
            print(f"   å‚æ•°: {tool_call['args']}")
            tool_called = True
    elif msg.__class__.__name__ == 'ToolMessage':
        print(f"âœ… å·¥å…·è¿”å›: {msg.content}")
        print()

if not tool_called:
    print("â„¹ï¸  æœ¬æ¬¡å¯¹è¯æœªè°ƒç”¨ä»»ä½•å·¥å…·ï¼ˆç›´æ¥å›å¤ï¼‰")

final_message = response['messages'][-1]
print("-" * 60)
print(f"\nğŸ’¬ Agent æœ€ç»ˆå›å¤: {final_message.content}\n")

# æå–ç»“æ„åŒ–è¾“å‡º
print("\nğŸ“Š ç»“æ„åŒ–è¾“å‡º (ResponseFormat):")
print("-" * 60)
structured = extract_structured_response(response)
print(f"  punny_response: \"{structured['punny_response']}\"")
print(f"  weather_conditions: {structured['weather_conditions']}")
print()

# ===== Run Example 3 =====
print("=" * 60)
print("Example 3: Different user")
print("=" * 60)

# æ”¹å˜ç”¨æˆ·ä¸Šä¸‹æ–‡
USER_CONTEXT["current_user"] = Context(user_id="2")

# æ–°çš„å¯¹è¯çº¿ç¨‹
config2 = {"configurable": {"thread_id": "2"}}

response = agent.invoke(
    {
        "messages": [
            SystemMessage(content=SYSTEM_PROMPT),
            HumanMessage(content="what is the weather where I am?")
        ]
    },
    config=config2
)

# åˆ†æå·¥å…·è°ƒç”¨
print("\nğŸ“‹ å·¥å…·è°ƒç”¨è®°å½•:")
print("-" * 60)
for msg in response['messages']:
    if hasattr(msg, 'tool_calls') and msg.tool_calls:
        for tool_call in msg.tool_calls:
            print(f"ğŸ”§ è°ƒç”¨å·¥å…·: {tool_call['name']}")
            print(f"   å‚æ•°: {tool_call['args']}")
    elif msg.__class__.__name__ == 'ToolMessage':
        print(f"âœ… å·¥å…·è¿”å›: {msg.content}")
        print()

final_message = response['messages'][-1]
print("-" * 60)
print(f"\nğŸ’¬ Agent æœ€ç»ˆå›å¤: {final_message.content}\n")

# æå–ç»“æ„åŒ–è¾“å‡º
print("\nğŸ“Š ç»“æ„åŒ–è¾“å‡º (ResponseFormat):")
print("-" * 60)
structured = extract_structured_response(response)
print(f"  punny_response: \"{structured['punny_response']}\"")
print(f"  weather_conditions: {structured['weather_conditions']}")
print()

print("=" * 60)
print("All examples completed!")
print("=" * 60)
